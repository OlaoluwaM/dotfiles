name: local-ai

services:
  ollama:
    container_name: ollama
    pull_policy: always
    build:
      context: ./ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "localhost:11434"]
      start_period: 30s
      interval: 1m
      timeout: 7s
      retries: 3
    environment:
      - OLLAMA_KEEP_ALIVE="10m"

  open-webui:
    container_name: open-webui
    depends_on:
      open-webui-mcpo:
        condition: service_started
        required: true
      ollama:
        condition: service_healthy
        required: false
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "3088:8080"
    volumes:
      - open-webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  open-webui-mcpo:
    container_name: open-webui-mcpo
    pull_policy: always
    build:
      context: ./mcpo
    restart: unless-stopped
    ports:
      - "8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - type: bind
        source: ./mcpo/data/memory.json
        target: /mcp-servers/memory.json
    environment:
      - MEMORY_FILE_PATH=/mcp-servers/memory.json

  vllm-reranker:
    container_name: vllm-reranker
    image: vllm/vllm-openai:latest
    pull_policy: always
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "8002:8000"  # Different port for reranking service
    volumes:
      - type: bind
        source: ~/.cache/huggingface
        target: /root/.cache/huggingface
        bind:
          create_host_path: true
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ipc: host
    command: [
      "--model", "Qwen/Qwen3-Reranker-0.6B",
      "--max-model-len", "8192",
      "--enforce-eager",
      "--hf_overrides", '{"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}'
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 90s
      interval: 30s
      timeout: 10s
      retries: 3


  # vllm-server:
  #   container_name: vllm-server
  #   image: vllm/vllm-openai:latest
  #   pull_policy: always
  #   restart: unless-stopped
  #   runtime: nvidia
  #   ports:
  #     - "8001:8000"  # Changed to 8001 to avoid conflict with open-webui-mcpo
  #   volumes:
  #     - type: bind
  #       source: ~/.cache/huggingface
  #       target: /root/.cache/huggingface
  #       bind:
  #         create_host_path: true
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   ipc: host
  #   command: ["--model", "Qwen/Qwen2.5-0.5B-Instruct"]  # Updated to a more recent model
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     start_period: 60s
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

volumes:
  ollama:
    name: "ollama-data"
  open-webui:
    name: "open-webui-data"
  open-webui-mcpo:
    name: "open-webui-mcpo-data"
  huggingface-cache:
    name: "huggingface-cache-data"

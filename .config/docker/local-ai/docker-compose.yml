name: local-ai

services:
  ollama:
    container_name: ollama
    pull_policy: always
    build:
      context: ./ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "localhost:11434"]
      start_period: 30s
      interval: 1m
      timeout: 7s
      retries: 3
    environment:
      - OLLAMA_KEEP_ALIVE="10m"

  open-webui:
    container_name: open-webui
    depends_on:
      open-webui-mcpo:
        condition: service_started
        required: true
      ollama:
        condition: service_healthy
        required: false
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "3088:8080"
    volumes:
      - open-webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  open-webui-mcpo:
    container_name: open-webui-mcpo
    pull_policy: always
    build:
      context: ./mcpo
    restart: unless-stopped
    ports:
      - "8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - type: bind
        source: ./mcpo/data/memory.json
        target: /mcp-servers/memory.json
    environment:
      - MEMORY_FILE_PATH=/mcp-servers/memory.json

  # vLLM doesn't support my specific Intel GPU, but my nvidia GPU works fine
  # vLLM only supports loading one model per server instance, so we might as well name this server by it's purpose
  vllm-reranker:
    container_name: vllm-reranker
    image: vllm/vllm-openai:latest
    pull_policy: always
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "8001:8000"
    volumes:
      - type: bind
        source: ~/.cache/huggingface/vllm/reranker/nvidia
        target: /root/.cache/huggingface
        bind:
          create_host_path: true
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: [
      "--model", "Qwen/Qwen3-Reranker-0.6B",
      "--max-model-len", "8192",
      # By default, vLLM pre-allocates up to 90% of your GPU VRAM for the model weights and KV cache to maximize throughput and support large batches or long contexts. This is controlled by the --gpu-memory-utilization flag. If you want vLLM to use less VRAM, lower this value. For now, 0.5 seems to be the lowest we can set it as anything below that just results to the same amount of memory usage as 0.5
      # https://github.com/vllm-project/vllm/issues/1062
      "--gpu-memory-utilization", "0.5",
      "--enforce-eager",
      "--enable-sleep-mode",
      "--hf_overrides", '{"architectures": ["Qwen3ForSequenceClassification"],"classifier_from_token": ["no", "yes"],"is_original_qwen3_reranker": true}'
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 90s
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama:
    name: "ollama-data"
  open-webui:
    name: "open-webui-data"
